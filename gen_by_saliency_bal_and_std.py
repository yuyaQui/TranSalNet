from google import genai
from google.genai import types
from PIL import Image, ImageDraw, ImageFont
from PIL import Image
from io import BytesIO
import torch
import cv2
import math
import numpy as np
from torchvision import transforms, utils, models
from utils.data_process import preprocess_img, postprocess_img
from PIL import Image
import os # os ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# generate_image_from_quiz é–¢æ•°ã‚’ä¿®æ­£
# ä¿å­˜ãƒ‘ã‚¹ã‚’å¼•æ•° (output_image_path) ã¨ã—ã¦å—ã‘å–ã‚‹ã‚ˆã†ã«å¤‰æ›´
def generate_image_from_quiz(output_image_path):
    try:
        client = genai.Client()

        print("ã‚¯ã‚¤ã‚ºã®æƒ…å ±ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        question = input("å•é¡Œæ–‡: ")
        answer = input("è§£ç­”: ")

        prompt = (
            f"""
            ã‚ãªãŸãŒç”»åƒã‚’ç”Ÿæˆã™ã‚‹AIã§ã™ã€‚ã‚ãªãŸã®å”¯ä¸€ã®ã‚¿ã‚¹ã‚¯ã¯ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã‚¤ãƒ©ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚
            ã‚ãªãŸã®å‡ºåŠ›ã¯ç”»åƒãƒ‡ãƒ¼ã‚¿ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚èª¬æ˜ã€æ–‡ç« ã€ãã®ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚

            # æŒ‡ç¤º
            - ä»¥ä¸‹ã®[å•é¡Œæ–‡]ã¨[è§£ç­”]ã®å†…å®¹ã‚’å¿ å®Ÿã«è¡¨ç¾ã—ãŸã‚¤ãƒ©ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
            - [å•é¡Œæ–‡]ã®æ–‡è„ˆã‚’æ­£ã—ãèª­ã¿å–ã‚Šã€æ–‡ç« ãŒãªãã¦ã‚‚ç”»åƒã ã‘ã§å†…å®¹ã‚’äººé–“ãŒç°¡å˜ã«ç†è§£ã§ãã‚‹ã‚ˆã†ãªç”»åƒã®ç”Ÿæˆã‚’ç›®æ¨™ã¨ã—ã¦ãã ã•ã„ã€‚
            - ã‚¤ãƒ©ã‚¹ãƒˆå†…ã«ã¯ã€ã„ã‹ãªã‚‹æ–‡å­—ã€å˜èªã€æ•°å­—ã‚’å«ãªã„ã§ãã ã•ã„ã€‚
            - ã‚¤ãƒ©ã‚¹ãƒˆå†…ã«ã¯ã€è‹±èªã‚’é…ç½®ã—ãªã„ã§ãã ã•ã„ã€‚
            - ã‚¤ãƒ©ã‚¹ãƒˆã¯ã€è¢«å†™ä½“ãŒå¤§ããæã‹ã‚Œã€èƒŒæ™¯ã®ç©ºç™½ãŒå°‘ãªããªã‚‹ã‚ˆã†ã«æ§‹æˆã—ã¦ãã ã•ã„ã€‚
            - ã‚¹ã‚¿ã‚¤ãƒ«: é©ã—ãŸã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©å®œåˆ¤æ–­

            # æƒ…å ±
            [å•é¡Œæ–‡]
            {question}

            [è§£ç­”]
            {answer}
            """
        )
        print("\nç”»åƒã‚’ç”Ÿæˆä¸­ã§ã™...ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚")

        response = client.models.generate_content(
            model="gemini-2.5-flash-image",
            contents=[prompt],
        )

        # ä¿®æ­£: response ã¨ response.candidates ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹
        if response and response.candidates:
            image_found = False
            for part in response.candidates[0].content.parts:
                if part.inline_data is not None:
                    # ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®å‡¦ç†
                    image = Image.open(BytesIO(part.inline_data.data)).convert("RGB")
                    
                    # â–¼â–¼â–¼ã€å¤‰æ›´ç‚¹ã€‘â–¼â–¼â–¼
                    # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚Œã¦ã„ãŸãƒ‘ã‚¹ã‚’ã€å¼•æ•°ã§å—ã‘å–ã£ãŸãƒ‘ã‚¹ã«å¤‰æ›´
                    output_filename = output_image_path
                    # â–²â–²â–²ã€å¤‰æ›´ç‚¹ã€‘â–²â–²â–²

                    # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã€ãªã‘ã‚Œã°ä½œæˆ
                    os.makedirs(os.path.dirname(output_filename), exist_ok=True)
                    
                    image.save(output_filename)
                    print(f"GeminiãŒç”Ÿæˆã—ãŸç”»åƒã‚’'{output_filename}'ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ")
                    image_found = True
                    return image, answer

            # ãƒ«ãƒ¼ãƒ—ãŒçµ‚äº†ã—ã¦ã‚‚ç”»åƒãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆ
            if not image_found:
                print("âŒ å¿œç­”ã«ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚")
                # ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ãŒã‚ã‚Œã°è¡¨ç¤ºã™ã‚‹ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                if response.candidates[0].content.parts and response.candidates[0].content.parts[0].text:
                    print(f"ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”: {response.candidates[0].content.parts[0].text}")
                return None, None
        else:
            # responseè‡ªä½“ãŒç„¡åŠ¹ã ã£ãŸå ´åˆ
            print("âŒ ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªå¿œç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None, None

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None, None

def distance(x, y, i, j):
    return int(math.sqrt((x - i)**2 + (y - j)**2)) # è·é›¢è¨ˆç®—é–¢æ•°ã‚’ä¿®æ­£

if __name__ == "__main__":

    # ==========================================================
    # âš™ï¸ è¨­å®šï¼ˆãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸå¤‰æ•°ï¼‰
    # ==========================================================
    
    # --- 1. ãƒ‘ã‚¹è¨­å®š ---
    OUTPUT_DIR = "example" # ãƒ¡ã‚¤ãƒ³ã®å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    
    # TranSalNetãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
    MODEL_PATH_RES = r'pretrained_models\TranSalNet_Res.pth'
    MODEL_PATH_DENSE = r'pretrained_models\TranSalNet_Dense.pth'

    # Windowsã®ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ï¼ˆç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰
    FONT_PATH = "C:/Windows/Fonts/meiryob.ttc" 
    
    # --- 2. ãƒ¢ãƒ‡ãƒ«é¸æŠ ---
    # 0 = TranSalNet_Dense, 1 = TranSalNet_Res
    MODEL_FLAG = 1 

    # --- 3. Saliencyãƒ”ãƒ¼ã‚¯æ¤œå‡ºè¨­å®š ---
    # ã“ã®Saliencyå€¤ã‚’è¶…ãˆã‚‹ãƒ”ã‚¯ã‚»ãƒ«ã‚’ã€Œé¡•è‘—ãªãƒ”ãƒ¼ã‚¯ã€ã¨ã—ã¦è€ƒæ…®ã™ã‚‹
    PEAK_SALIENCY_THRESHOLD = 180
    
    # æ¤œå‡ºã™ã‚‹ãƒ”ãƒ¼ã‚¯ã®æœ€å¤§æ•°
    MAX_PEAKS_TO_FIND = 5
    MIN_PEAKS_TO_FIND = 2
    
    # ãƒ”ãƒ¼ã‚¯å‘¨è¾ºã‚’ãƒã‚¹ã‚¯ã™ã‚‹éš›ã®Saliencyã—ãã„å€¤
    # ã“ã®ã—ãã„å€¤ã‚’ä¸‹å›ã‚‹éƒ¨åˆ†ã¾ã§ã®è·é›¢ã§ãƒã‚¹ã‚¯åŠå¾„ã‚’æ±ºå®š
    MASK_SALIENCY_THRESHOLD_FOR_RADIUS = 100 
    
    # ãƒã‚¹ã‚¯åŠå¾„ã®æœ€å°å€¤ï¼ˆãƒ”ãƒ¼ã‚¯å‘¨è¾ºã‚’ç¢ºå®Ÿã«ãƒã‚¹ã‚¯ã™ã‚‹ãŸã‚ï¼‰
    MIN_MASK_RADIUS = 50

    POS_OFFSET = 20

    # --- 4. ãƒ†ã‚­ã‚¹ãƒˆæç”»è¨­å®š ---
    FONT_SIZE = 36
    FILL_COLOR = "#00ff00" # ãƒ†ã‚­ã‚¹ãƒˆæœ¬ä½“ã®è‰² (ç·‘)
    STROKE_COLOR = "black"  # ç¸å–ã‚Šã®è‰² (é»’)
    STROKE_WIDTH = 3        # ç¸å–ã‚Šã®å¤ªã•

    # ==========================================================
    # å‡¦ç†é–‹å§‹
    # ==========================================================

    # è¨­å®šã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
    generated_image_path = os.path.join(OUTPUT_DIR, "generated_image.png")
    saliency_output_filename = os.path.join(OUTPUT_DIR, "result_saliency.png")
    final_output_filename = os.path.join(OUTPUT_DIR, "final_result_with_answer.png")

    # 1. ç”»åƒç”Ÿæˆï¼ˆå¼•æ•°ã«ä¿å­˜ãƒ‘ã‚¹ã‚’æ¸¡ã™ï¼‰
    generated_pil_image, answer_text = generate_image_from_quiz(generated_image_path)

    if generated_pil_image is None:
        print("ç”»åƒã®ç”Ÿæˆã«å¤±æ•—ã—ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
    else:
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        print(f"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")

        if MODEL_FLAG == 1:
            from TranSalNet_Res import TranSalNet
            model = TranSalNet()
            model.load_state_dict(torch.load(MODEL_PATH_RES))
        else:
            from TranSalNet_Dense import TranSalNet
            model = TranSalNet()
            model.load_state_dict(torch.load(MODEL_PATH_DENSE))

        # --- 2-1. Saliency Mapè¨ˆç®—ã®æº–å‚™ ---
        model = model.to(device) 
        model.eval()

        # --- 2-2. å…¥åŠ›ç”»åƒã®å‰å‡¦ç† ---
        img = preprocess_img(generated_image_path) # ç”»åƒã‚’ã‚¯ãƒ­ãƒƒãƒ—&Numpyã«å¤‰æ›
        img = np.array(img)/255.
        img = np.expand_dims(np.transpose(img,(2,0,1)),axis=0)

        # --- 2-3. PyTorchãƒ†ãƒ³ã‚½ãƒ«ã¸ã®å¤‰æ›ã¨æ¨è«– ---
        img = torch.from_numpy(img).type(torch.cuda.FloatTensor).to(device)
        pred_saliency_tensor = model(img)

        # --- 2-4. å‡ºåŠ›ã®å¾Œå‡¦ç† ---
        toPIL = transforms.ToPILImage()
        pic = toPIL(pred_saliency_tensor.squeeze())
        saliency_map_np = postprocess_img(pic, generated_image_path) # Numpyé…åˆ—ã«å¤‰æ›
        
        # --- 2-5. é¡•è‘—æ€§ãƒãƒƒãƒ—ã®ä¿å­˜ ---
        cv2.imwrite(saliency_output_filename, saliency_map_np, [int(cv2.IMWRITE_JPEG_QUALITY), 100])
        print(f"Saliency Mapã‚’ '{saliency_output_filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")
        
        # å…ƒç”»åƒã®ã‚µã‚¤ã‚ºã‚’ã“ã“ã§å–å¾—
        img_width, img_height = generated_pil_image.size
        h, w = saliency_map_np.shape

        # â–¼â–¼â–¼â–¼â–¼ã€ãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£ç®‡æ‰€ã€‘â–¼â–¼â–¼â–¼â–¼
        # --- 2-6. åå¾©çš„ãªãƒ”ãƒ¼ã‚¯æ¤œå‡ºã¨ãƒã‚¹ã‚¯å‡¦ç† ---
        print(f"\nğŸ” åå¾©çš„ãªãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’é–‹å§‹ (æœ€å°{MIN_PEAKS_TO_FIND}å€‹ã€æœ€å¤§{MAX_PEAKS_TO_FIND}å€‹ã€ã—ãã„å€¤ > {PEAK_SALIENCY_THRESHOLD})")

        # ãƒ”ãƒ¼ã‚¯ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
        all_x_peaks = []
        all_y_peaks = []
        all_vals_peaks = []

        # ãƒã‚¹ã‚¯å‡¦ç†ç”¨ã®Saliencyãƒãƒƒãƒ—ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
        temp_map = saliency_map_np.copy()
        
        # å…¨åº§æ¨™ (Y, X) ã‚°ãƒªãƒƒãƒ‰ã‚’äº‹å‰ã«è¨ˆç®—
        Y_grid, X_grid = np.ogrid[:h, :w]
        
        # ã—ãã„å€¤ã‚’ä¸‹å›ã‚‹ãƒ”ã‚¯ã‚»ãƒ«ã®åº§æ¨™ï¼ˆãƒã‚¹ã‚¯åŠå¾„ã®è¨ˆç®—ç”¨ã€ã“ã‚Œã¯ãƒ«ãƒ¼ãƒ—ã®å¤–ã§1å›ã ã‘è¨ˆç®—ï¼‰
        y_low, x_low = np.where(saliency_map_np < MASK_SALIENCY_THRESHOLD_FOR_RADIUS)
        
        # MAX_PEAKS_TO_FIND ã®å›æ•°ã ã‘ãƒ«ãƒ¼ãƒ—
        for i in range(MAX_PEAKS_TO_FIND):
            # 1. ç¾åœ¨ã®ãƒãƒƒãƒ— (temp_map) ã‹ã‚‰æœ€å¤§å€¤ï¼ˆãƒ”ãƒ¼ã‚¯Nï¼‰ã‚’è¦‹ã¤ã‘ã‚‹
            val_n = np.max(temp_map)
            
            # 2. ãƒ”ãƒ¼ã‚¯ã®Saliencyå€¤ãŒã—ãã„å€¤ã‚’ä¸‹å›ã£ãŸã‚‰ã€ãƒ«ãƒ¼ãƒ—çµ‚äº†
            if val_n < PEAK_SALIENCY_THRESHOLD and i > MIN_PEAKS_TO_FIND - 1:
                print(f"  æ¬¡ã®ãƒ”ãƒ¼ã‚¯ã®å€¤ ({val_n:.2f}) ãŒã—ãã„å€¤ ({PEAK_SALIENCY_THRESHOLD}) ã‚’ä¸‹å›ã£ãŸãŸã‚ã€æ¤œå‡ºã‚’çµ‚äº†ã€‚")
                break
                
            loc_n_flat = np.argmax(temp_map) 
            y_n, x_n = np.unravel_index(loc_n_flat, temp_map.shape)
            
            print(f"  ãƒ”ãƒ¼ã‚¯{i+1}: (x={x_n}, y={y_n}) (å€¤: {val_n:.2f}) ã‚’æ¤œå‡ºã€‚")
            
            # 3. è¦‹ã¤ã‹ã£ãŸãƒ”ãƒ¼ã‚¯ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
            all_x_peaks.append(x_n)
            all_y_peaks.append(y_n)
            all_vals_peaks.append(val_n)

            # 4. ã€åå¾©å‡¦ç†ã€‘ãƒ”ãƒ¼ã‚¯N (x_n, y_n) ã®å‘¨è¾ºã‚’ãƒã‚¹ã‚¯ã™ã‚‹ãŸã‚ã®åŠå¾„ã‚’è¨ˆç®—
            mask_radius = MIN_MASK_RADIUS # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æœ€å°åŠå¾„
            if y_low.size > 0:
                # ã—ãã„å€¤ã‚’ä¸‹å›ã‚‹å…¨ãƒ”ã‚¯ã‚»ãƒ«ã¨ã€ä»Šè¦‹ã¤ã‘ãŸãƒ”ãƒ¼ã‚¯N (x_n, y_n) ã¨ã®è·é›¢ã‚’è¨ˆç®—
                distances_to_low = np.sqrt((x_low - x_n)**2 + (y_low - y_n)**2)
                calculated_radius = np.min(distances_to_low)
                # æœ€å°åŠå¾„ã¨æ¯”è¼ƒã—ã¦å¤§ãã„æ–¹ã‚’æ¡ç”¨
                mask_radius = max(calculated_radius, MIN_MASK_RADIUS)
            else:
                # ã—ãã„å€¤ã‚’ä¸‹å›ã‚‹ãƒ”ã‚¯ã‚»ãƒ«ãŒãªã„ï¼ˆç”»åƒå…¨ä½“ãŒæ˜ã‚‹ã„ï¼‰å ´åˆ
                print(f"  (åŠå¾„è¨ˆç®—) Saliency < {MASK_SALIENCY_THRESHOLD_FOR_RADIUS} ã®é ˜åŸŸãªã—ã€‚æœ€å°åŠå¾„({MIN_MASK_RADIUS})ã‚’ä½¿ç”¨ã€‚")
        
            # 5. ã€åå¾©å‡¦ç†ã€‘temp_map ä¸Šã®ãƒ”ãƒ¼ã‚¯Nã®å‘¨è¾ºã‚’ãƒã‚¹ã‚¯ï¼ˆæ¶ˆå»ï¼‰ã™ã‚‹
            dist_from_peak_n = np.sqrt((X_grid - x_n)**2 + (Y_grid - y_n)**2)
            temp_map[dist_from_peak_n <= mask_radius] = 0
            # print(f"  ãƒ”ãƒ¼ã‚¯{i+1}ã®å‘¨å›² (åŠå¾„ {mask_radius:.2f}) ã‚’ãƒã‚¹ã‚¯ã—ã¾ã—ãŸã€‚")

        # --- 2-7. æ¤œå‡ºã•ã‚ŒãŸå…¨ãƒ”ãƒ¼ã‚¯ã®Saliencyå€¤ã§åŠ é‡å¹³å‡ï¼ˆå†…åˆ†ç‚¹ï¼‰ã‚’è¨ˆç®— ---

        # ãƒªã‚¹ãƒˆã‚’Numpyé…åˆ—ã«å¤‰æ›
        final_x_peaks = np.array(all_x_peaks)
        final_y_peaks = np.array(all_y_peaks)
        final_vals_peaks = np.array(all_vals_peaks)

        print("\n=== æœ€çµ‚çš„ãªãƒ†ã‚­ã‚¹ãƒˆé…ç½®ã®è¨ˆç®—ã«ä½¿ç”¨ã™ã‚‹ãƒ”ãƒ¼ã‚¯ ===")
        
        if final_x_peaks.size == 0:
            # ãƒ”ãƒ¼ã‚¯ãŒä¸€ã¤ã‚‚è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆ (ã—ãã„å€¤ãŒé«˜ã™ãã‚‹ãªã©)
            print(f"âš ï¸ Saliency > {PEAK_SALIENCY_THRESHOLD} ã®ãƒ”ãƒ¼ã‚¯ãŒ0å€‹ã§ã—ãŸã€‚Saliencyæœ€å¤§å€¤ã‚’ãƒ”ãƒ¼ã‚¯ã¨ã—ã¾ã™ã€‚")
            val1 = np.max(saliency_map_np)
            loc1_flat = np.argmax(saliency_map_np)
            y1, x1 = np.unravel_index(loc1_flat, saliency_map_np.shape)
            
            final_x_peaks = np.array([x1])
            final_y_peaks = np.array([y1])
            final_vals_peaks = np.array([val1])

        for i in range(final_x_peaks.size):
            print(f"  ãƒ”ãƒ¼ã‚¯{i+1}: (x={final_x_peaks[i]}, y={final_y_peaks[i]}) (å€¤: {final_vals_peaks[i]:.2f})")

        # é‡ã¿ã®åˆè¨ˆã‚’è¨ˆç®— (ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å›é¿ã®ãŸã‚floatã§)
        total_weight = np.sum(final_vals_peaks.astype(float))

        if total_weight > 0:
            # xåº§æ¨™ã¨yåº§æ¨™ã®åŠ é‡å¹³å‡ã‚’è¨ˆç®—
            weighted_x_sum = np.sum(final_x_peaks.astype(float) * final_vals_peaks.astype(float))
            weighted_y_sum = np.sum(final_y_peaks.astype(float) * final_vals_peaks.astype(float))
            
            final_x = int(weighted_x_sum / total_weight)
            final_y = int(weighted_y_sum / total_weight)
            
            print(f"âœ… {final_x_peaks.size}å€‹ã®ãƒ”ãƒ¼ã‚¯ã®åŠ é‡å¹³å‡ã‚’è¨ˆç®—ã€‚")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé€šå¸¸ã¯ç™ºç”Ÿã—ãªã„ãŒã€0é™¤ç®—ã‚’é¿ã‘ã‚‹ï¼‰
            print(f"âš ï¸ é‡ã¿ã®åˆè¨ˆãŒ0ã§ã™ã€‚ç”»åƒã®ä¸­å¿ƒã‚’ä½¿ã„ã¾ã™ã€‚")
            final_x = img_width // 2
            final_y = img_height // 2
        # â–²â–²â–²â–²â–²ã€ã“ã“ã¾ã§ãŒå¤‰æ›´ç®‡æ‰€ã€‘â–²â–²â–²â–²â–²
        
        print(f"ğŸ’¡ æœ€é©ãªãƒ†ã‚­ã‚¹ãƒˆé…ç½®åº§æ¨™ (x, y): ({final_x}, {final_y})")

        draw = ImageDraw.Draw(generated_pil_image)
        
        # ãƒ•ã‚©ãƒ³ãƒˆã¨ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã®æº–å‚™
        try:
            font = ImageFont.truetype(FONT_PATH, FONT_SIZE)
        except IOError:
            print(f"è­¦å‘Š: æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ³ãƒˆ '{FONT_PATH}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            font = ImageFont.load_default()
        
        text_bbox = draw.textbbox((0, 0), answer_text, font=font)
        text_width = text_bbox[2] - text_bbox[0]
        text_height = text_bbox[3] - text_bbox[1]

        text_half_width = text_width // 2
        text_half_height = text_height // 2

        left = final_x - POS_OFFSET
        top = final_y - POS_OFFSET
        right = final_x + POS_OFFSET
        bottom = final_y + POS_OFFSET

        factor_flag = float('inf')

        print(f"\nğŸ” æ³¨ç›®é ˜åŸŸ ({left}, {top}) ã‹ã‚‰ ({right}, {bottom}) ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã€æœ€é©ãªãƒ†ã‚­ã‚¹ãƒˆé…ç½®ä½ç½®ã‚’æ¢ã—ã¾ã™...")

        for i in range(left, right):
            for j in range(top, bottom):
                crop_left = max(0, i - text_half_width)
                crop_top = max(0, j - text_half_height)
                crop_right = min(img_width, i + text_half_width)
                crop_bottom = min(img_height, j + text_half_height)
                
                if crop_left < crop_right and crop_top < crop_bottom:
                    patch_pil = generated_pil_image.crop((crop_left, crop_top, crop_right, crop_bottom))
                    patch_gray = patch_pil.convert("L")
                    patch_np = np.array(patch_gray)
                    
                    if patch_np.size == 0:
                        continue

                    intensity_std = np.std(patch_np)

                    if (intensity_std < factor_flag):
                        factor_flag = intensity_std
                        final_x, final_y = i, j

        print(f"âœ… ã‚¹ã‚­ãƒ£ãƒ³å®Œäº†ã€‚")
        print(f"ğŸ’¡ æœ€é©ãªãƒ†ã‚­ã‚¹ãƒˆé…ç½®åº§æ¨™ (x, y): ({final_x}, {final_y})")

        # ----- 3. å…ƒç”»åƒã«è§£ç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’æç”» -----
        
        # æœ€çµ‚çš„ãªåº§æ¨™ (final_x, final_y) ã‚’ä¸­å¿ƒã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æç”»
        text_x = final_x - text_half_width
        text_y = final_y - text_half_height
        
        # ãƒ†ã‚­ã‚¹ãƒˆãŒç”»åƒã‹ã‚‰ã¯ã¿å‡ºã•ãªã„ã‚ˆã†ã«åº§æ¨™ã‚’æœ€çµ‚èª¿æ•´
        if text_x < 0:
            text_x = 0
        if text_x + text_width > img_width:
            text_x = img_width - text_width
        if text_y < 0:
            text_y = 0
        if text_y + text_height > img_height:
            text_y = img_height - text_height
        
        draw.text(
            (text_x, text_y),
            answer_text,
            font=font, 
            fill=FILL_COLOR,
            stroke_width=STROKE_WIDTH,
            stroke_fill=STROKE_COLOR
        )

        # ----- 4. æœ€çµ‚ç”»åƒã‚’ä¿å­˜ -----
        generated_pil_image.save(final_output_filename)
        print(f"ğŸ‰ å®Œæˆï¼è§£ç­”ãƒ†ã‚­ã‚¹ãƒˆä»˜ãç”»åƒã‚’ '{final_output_filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")