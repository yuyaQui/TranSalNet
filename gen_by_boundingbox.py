from google import genai
from google.genai import types
from PIL import Image, ImageDraw, ImageFont
from PIL import Image
from io import BytesIO
import torch
import cv2
import math
import numpy as np
from torchvision import transforms, utils, models
from utils.data_process import preprocess_img, postprocess_img
from PIL import Image
import os # os ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

def generate_image_from_quiz():
    try:
        client = genai.Client()

        print("ã‚¯ã‚¤ã‚ºã®æƒ…å ±ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        question = input("å•é¡Œæ–‡: ")
        answer = input("è§£ç­”: ")

        prompt = (
            f"""
            ã‚ãªãŸãŒç”»åƒã‚’ç”Ÿæˆã™ã‚‹AIã§ã™ã€‚ã‚ãªãŸã®å”¯ä¸€ã®ã‚¿ã‚¹ã‚¯ã¯ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã‚¤ãƒ©ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚
            ã‚ãªãŸã®å‡ºåŠ›ã¯ç”»åƒãƒ‡ãƒ¼ã‚¿ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚èª¬æ˜ã€æ–‡ç« ã€ãã®ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚

            # æŒ‡ç¤º
            - ä»¥ä¸‹ã®[å•é¡Œæ–‡]ã¨[è§£ç­”]ã®å†…å®¹ã‚’å¿ å®Ÿã«è¡¨ç¾ã—ãŸã‚¤ãƒ©ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
            - [å•é¡Œæ–‡]ã®æ–‡è„ˆã‚’æ­£ã—ãèª­ã¿å–ã‚Šã€æ–‡ç« ãŒãªãã¦ã‚‚ç”»åƒã ã‘ã§å†…å®¹ã‚’äººé–“ãŒç°¡å˜ã«ç†è§£ã§ãã‚‹ã‚ˆã†ãªç”»åƒã®ç”Ÿæˆã‚’ç›®æ¨™ã¨ã—ã¦ãã ã•ã„ã€‚
            - ã‚¤ãƒ©ã‚¹ãƒˆå†…ã«ã¯ã€ã„ã‹ãªã‚‹æ–‡å­—ã€å˜èªã€æ•°å­—ã‚’å«ã¾ãªã„ã§ãã ã•ã„ã€‚
            - ã‚¤ãƒ©ã‚¹ãƒˆã¯ã€è¢«å†™ä½“ãŒå¤§ããæã‹ã‚Œã€èƒŒæ™¯ã®ç©ºç™½ãŒå°‘ãªããªã‚‹ã‚ˆã†ã«æ§‹æˆã—ã¦ãã ã•ã„ã€‚
            - ã‚¹ã‚¿ã‚¤ãƒ«: é©ã—ãŸã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©å®œåˆ¤æ–­

            # æƒ…å ±
            [å•é¡Œæ–‡]
            {question}

            [è§£ç­”]
            {answer}
            """
        )
        print("\nç”»åƒã‚’ç”Ÿæˆä¸­ã§ã™...ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚")

        response = client.models.generate_content(
            model="gemini-2.5-flash-image",
            contents=[prompt],
        )

        # ä¿®æ­£: response ã¨ response.candidates ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹
        if response and response.candidates:
            image_found = False
            for part in response.candidates[0].content.parts:
                if part.inline_data is not None:
                    # ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®å‡¦ç†
                    image = Image.open(BytesIO(part.inline_data.data)).convert("RGB")
                    output_filename = r"example/generated_image.png"
                    image.save(output_filename)
                    print(f"GeminiãŒç”Ÿæˆã—ãŸç”»åƒã‚’'{output_filename}'ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ")
                    image_found = True
                    return image, answer

            # ãƒ«ãƒ¼ãƒ—ãŒçµ‚äº†ã—ã¦ã‚‚ç”»åƒãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆ
            if not image_found:
                print("âŒ å¿œç­”ã«ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚")
                # ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ãŒã‚ã‚Œã°è¡¨ç¤ºã™ã‚‹ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                if response.candidates[0].content.parts and response.candidates[0].content.parts[0].text:
                    print(f"ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”: {response.candidates[0].content.parts[0].text}")
                return None, None
        else:
            # responseè‡ªä½“ãŒç„¡åŠ¹ã ã£ãŸå ´åˆ
            print("âŒ ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªå¿œç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None, None

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None, None

def distance(x, y, i, j):
    return int(math.sqrt((x - i)**2 + (y - j)**2)) # è·é›¢è¨ˆç®—é–¢æ•°ã‚’ä¿®æ­£

if __name__ == "__main__":

    generated_pil_image, answer_text = generate_image_from_quiz()

    if generated_pil_image is None:
        print("ç”»åƒã®ç”Ÿæˆã«å¤±æ•—ã—ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
    else:
        generated_image_path = "example/generated_image.png" # generate_image_from_quizã®å‡ºåŠ›ãƒ‘ã‚¹ã¨ä¸€è‡´ã•ã›ã‚‹
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        print(f"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")

        flag = 1 # 0 for TranSalNet_Dense, 1 for TranSalNet_Res

        if flag:
            from TranSalNet_Res import TranSalNet
            model = TranSalNet()
            model.load_state_dict(torch.load(r'pretrained_models\TranSalNet_Res.pth'))
        else:
            from TranSalNet_Dense import TranSalNet
            model = TranSalNet()
            model.load_state_dict(torch.load(r'pretrained_models\TranSalNet_Dense.pth'))

        # --- 2-1. Saliency Mapè¨ˆç®—ã®æº–å‚™ ---
        model = model.to(device) 
        model.eval()

        # --- 2-2. å…¥åŠ›ç”»åƒã®å‰å‡¦ç† ---
        img = preprocess_img(generated_image_path) # ç”»åƒã‚’ã‚¯ãƒ­ãƒƒãƒ—&Numpyã«å¤‰æ›
        img = np.array(img)/255.
        img = np.expand_dims(np.transpose(img,(2,0,1)),axis=0)

        # --- 2-3. PyTorchãƒ†ãƒ³ã‚½ãƒ«ã¸ã®å¤‰æ›ã¨æ¨è«– ---
        img = torch.from_numpy(img).type(torch.cuda.FloatTensor).to(device)
        pred_saliency_tensor = model(img)

        # --- 2-4. å‡ºåŠ›ã®å¾Œå‡¦ç† ---
        toPIL = transforms.ToPILImage()
        pic = toPIL(pred_saliency_tensor.squeeze())
        saliency_map_np = postprocess_img(pic, generated_image_path) # Numpyé…åˆ—ã«å¤‰æ›
        
        # --- 2-5. é¡•è‘—æ€§ãƒãƒƒãƒ—ã®ä¿å­˜ ---
        saliency_output_filename = r'example/result_saliency.png'
        cv2.imwrite(saliency_output_filename, saliency_map_np, [int(cv2.IMWRITE_JPEG_QUALITY), 100])
        print(f"Saliency Mapã‚’ '{saliency_output_filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")

        # --- 2-6. Saliency Mapã®æœ€å¤§å€¤ï¼ˆæœ€ã‚‚ç›®ç«‹ã¤ç‚¹ï¼‰ã‚’å–å¾— ---
        max_loc_flat = np.argmax(saliency_map_np)
        max_y, max_x = np.unravel_index(max_loc_flat, saliency_map_np.shape)
        print(f"ğŸ’¡ Saliency Mapã§æœ€ã‚‚æ³¨è¦–ã•ã‚Œã‚‹åº§æ¨™ (x, y): ({max_x}, {max_y})")

        saliency_center_x = int(max_x)
        saliency_center_y = int(max_y)

        saliency_threshold = 50
        print(f"\n Saliencyã®é–¾å€¤ã‚’{saliency_threshold}ã¨ã—ã¦åˆ‡ã‚Šå‡ºã—é ˜åŸŸã‚’è¨ˆç®—ã—ã¾ã™")

        y_coords, x_coords = np.where(saliency_map_np > saliency_threshold)

        top = float('inf')
        bottom = 0
        left = float('inf')
        right = 0

        if y_coords.size > 0: # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®é ‚ç‚¹
            top = np.min(y_coords)
            bottom = np.max(y_coords)
            left = np.min(x_coords)
            right = np.max(x_coords)
        else:
            print(f"é–¾å€¤{saliency_threshold}ã‚’è¶…ãˆã‚‹Saliencyé ˜åŸŸãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        draw = ImageDraw.Draw(generated_pil_image)
        
        # ãƒ•ã‚©ãƒ³ãƒˆã¨ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã®æº–å‚™
        font_path = "C:/Windows/Fonts/meiryob.ttc"
        font_size = 36
        try:
            font = ImageFont.truetype(font_path, font_size)
        except IOError:
            print(f"è­¦å‘Š: æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ³ãƒˆ '{font_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            font = ImageFont.load_default()
        
        text_bbox = draw.textbbox((0, 0), answer_text, font=font)
        text_width = text_bbox[2] - text_bbox[0]
        text_height = text_bbox[3] - text_bbox[1]

        text_half_width = text_width // 2
        text_half_height = text_height // 2

        img_width, img_height = generated_pil_image.size

        gray_std_threshold = 1
        gray_flag_var = float('inf')
        final_x, final_y = saliency_center_x, saliency_center_y

        for i in range(left, right):
            for j in range(top, bottom):
                crop_left = max(0, i - text_half_width)
                crop_top = max(0, j - text_half_height)
                crop_right = min(img_width, i + text_half_width)
                crop_bottom = min(img_height, j + text_half_height)

                if crop_left < crop_right and crop_top < crop_bottom:
                    patch_pil = generated_pil_image.crop((crop_left, crop_top, crop_right, crop_bottom))
                    patch_gray = patch_pil.convert("L")
                    patch_np = np.array(patch_gray)

                    if patch_np.size == 0:
                        continue

                    gray_sum = np.sum(patch_np)
                    gray_std = np.std(patch_np)
                    factor_var = gray_sum + gray_std

                    if gray_std < gray_std_threshold:
                        continue

                    if factor_var < gray_flag_var:
                        gray_flag_var = factor_var
                        final_x, final_y = i, j
        
        print(f"âœ… ã‚¹ã‚­ãƒ£ãƒ³å®Œäº†ã€‚")
        print(f"ğŸ’¡ æœ€é©ãªãƒ†ã‚­ã‚¹ãƒˆé…ç½®åº§æ¨™ (x, y): ({final_x}, {final_y})")

        text_x = final_x - text_half_width
        text_y = final_y - text_half_height

        if text_x < 0:
            text_x = 0
        if text_x > img_width - text_half_width * 2:
            text_x = img_width - text_half_width * 2
        if text_y < 0:
            text_y = 0
        if text_y > img_height + text_half_height * 2:
            text_y = img_height + text_half_height * 2
        
        fill_color = "#00ff00"
        stroke_color = "black"
        stroke_width = 3
        draw.text(
            (text_x, text_y),
            answer_text,
            font=font, 
            fill=fill_color,
            stroke_width=stroke_width,
            stroke_fill=stroke_color
        )

        final_output_filename = r"example/final_result_with_answer.png"
        generated_pil_image.save(final_output_filename)
        print(f"ğŸ‰ å®Œæˆï¼è§£ç­”ãƒ†ã‚­ã‚¹ãƒˆä»˜ãç”»åƒã‚’ '{final_output_filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")