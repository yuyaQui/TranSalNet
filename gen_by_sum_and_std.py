from google import genai
from google.genai import types
from PIL import Image, ImageDraw, ImageFont
from PIL import Image
from io import BytesIO
import torch
import cv2
import math
import numpy as np
from torchvision import transforms, utils, models
from utils.data_process import preprocess_img, postprocess_img
from PIL import Image
import os # os ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

def generate_image_from_quiz():
    try:
        client = genai.Client()

        print("ã‚¯ã‚¤ã‚ºã®æƒ…å ±ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        question = input("å•é¡Œæ–‡: ")
        answer = input("è§£ç­”: ")

        prompt = (
            f"""
            ã‚ãªãŸã¯ç”»åƒã‚’ç”Ÿæˆã™ã‚‹AIã§ã™ã€‚ã‚ãªãŸã®å”¯ä¸€ã®ã‚¿ã‚¹ã‚¯ã¯ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã‚¤ãƒ©ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚
            ã‚ãªãŸã®å‡ºåŠ›ã¯ç”»åƒãƒ‡ãƒ¼ã‚¿ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚èª¬æ˜ã€æ–‡ç« ã€ãã®ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚

            # æŒ‡ç¤º
            - ä»¥ä¸‹ã®[å•é¡Œæ–‡]ã¨[è§£ç­”]ã®å†…å®¹ã‚’å¿ å®Ÿã«è¡¨ç¾ã—ãŸã‚¤ãƒ©ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
            - [å•é¡Œæ–‡]ã®æ–‡è„ˆã‚’æ­£ã—ãèª­ã¿å–ã‚Šã€æ–‡ç« ãŒãªãã¦ã‚‚ç”»åƒã ã‘ã§å†…å®¹ã‚’äººé–“ãŒç°¡å˜ã«ç†è§£ã§ãã‚‹ã‚ˆã†ãªç”»åƒã®ç”Ÿæˆã‚’ç›®æ¨™ã¨ã—ã¦ãã ã•ã„ã€‚
            - ã‚¤ãƒ©ã‚¹ãƒˆå†…ã«ã¯ã€ã„ã‹ãªã‚‹æ–‡å­—ã€å˜èªã€æ•°å­—ã‚’å«ã¾ãªã„ã§ãã ã•ã„ã€‚
            - ã‚¤ãƒ©ã‚¹ãƒˆã¯ã€è¢«å†™ä½“ãŒå¤§ããæã‹ã‚Œã€èƒŒæ™¯ã®ç©ºç™½ãŒå°‘ãªããªã‚‹ã‚ˆã†ã«æ§‹æˆã—ã¦ãã ã•ã„ã€‚
            - ã‚¹ã‚¿ã‚¤ãƒ«: é©ã—ãŸã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©å®œåˆ¤æ–­

            # æƒ…å ±
            [å•é¡Œæ–‡]
            {question}

            [è§£ç­”]
            {answer}
            """
        )
        print("\nç”»åƒã‚’ç”Ÿæˆä¸­ã§ã™...ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚")

        response = client.models.generate_content(
            model="gemini-2.5-flash-image",
            contents=[prompt],
        )

        # ä¿®æ­£: response ã¨ response.candidates ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹
        if response and response.candidates:
            image_found = False
            for part in response.candidates[0].content.parts:
                if part.inline_data is not None:
                    # ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®å‡¦ç†
                    image = Image.open(BytesIO(part.inline_data.data)).convert("RGB")
                    output_filename = "generated_image.png"
                    image.save(output_filename)
                    print(f"GeminiãŒç”Ÿæˆã—ãŸç”»åƒã‚’'{output_filename}'ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ")
                    image_found = True
                    return image, answer

            # ãƒ«ãƒ¼ãƒ—ãŒçµ‚äº†ã—ã¦ã‚‚ç”»åƒãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆ
            if not image_found:
                print("âŒ å¿œç­”ã«ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚")
                # ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ãŒã‚ã‚Œã°è¡¨ç¤ºã™ã‚‹ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                if response.candidates[0].content.parts and response.candidates[0].content.parts[0].text:
                    print(f"ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”: {response.candidates[0].content.parts[0].text}")
                return None, None
        else:
            # responseè‡ªä½“ãŒç„¡åŠ¹ã ã£ãŸå ´åˆ
            print("âŒ ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªå¿œç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None, None

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None, None

def distance(x, y, i, j):
    return int(math.sqrt())

if __name__ == "__main__":

    generated_pil_image, answer_text = generate_image_from_quiz()

    if generated_pil_image is None:
        print("ç”»åƒã®ç”Ÿæˆã«å¤±æ•—ã—ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
    else:
        generated_image_path = "generated_image.png"
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        print(f"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")

        flag = 1 # 0 for TranSalNet_Dense, 1 for TranSalNet_Res

        if flag:
            from TranSalNet_Res import TranSalNet
            model = TranSalNet()
            model.load_state_dict(torch.load(r'pretrained_models\TranSalNet_Res.pth'))
        else:
            from TranSalNet_Dense import TranSalNet
            model = TranSalNet()
            model.load_state_dict(torch.load(r'pretrained_models\TranSalNet_Dense.pth'))

        model = model.to(device) 
        model.eval()

        img = preprocess_img(generated_image_path)
        img = np.array(img)/255.
        img = np.expand_dims(np.transpose(img,(2,0,1)),axis=0)
        img = torch.from_numpy(img).type(torch.cuda.FloatTensor).to(device)
        pred_saliency_tensor = model(img)
        toPIL = transforms.ToPILImage()
        pic = toPIL(pred_saliency_tensor.squeeze())

        saliency_map_np = postprocess_img(pic, generated_image_path)
        saliency_output_filename = r'example/result_saliency.png'
        cv2.imwrite(saliency_output_filename, saliency_map_np, [int(cv2.IMWRITE_JPEG_QUALITY), 100])
        print(f"Saliency Mapã‚’ '{saliency_output_filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")

        max_loc_flat = np.argmax(saliency_map_np)
        max_y, max_x = np.unravel_index(max_loc_flat, saliency_map_np.shape)
        print(f"ğŸ’¡ Saliency Mapã§æœ€ã‚‚æ³¨è¦–ã•ã‚Œã‚‹åº§æ¨™ (x, y): ({max_x}, {max_y})")

        saliency_center_x = int(max_x)
        saliency_center_y = int(max_y)

        square_size = 100
        
        half_size = square_size // 2

        left = saliency_center_x - half_size
        top = saliency_center_y - half_size
        right = saliency_center_x + half_size
        bottom = saliency_center_y + half_size

        draw = ImageDraw.Draw(generated_pil_image)
        
        font_path = "C:/Windows/Fonts/meiryob.ttc"
        font_size = 36
        try:
            font = ImageFont.truetype(font_path, font_size)
        except IOError:
            print(f"è­¦å‘Š: æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ³ãƒˆ '{font_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            font = ImageFont.load_default()
        


        text_bbox = draw.textbbox((0, 0), answer_text, font=font)
        text_width = text_bbox[2] - text_bbox[0]
        text_height = text_bbox[3] - text_bbox[1]
        text_half_width = text_width // 2
        text_half_height = text_height // 2

        img_width, img_height = generated_pil_image.size
        
        flagVar = float('inf') 
        final_x, final_y = saliency_center_x, saliency_center_y

        print(f"\nğŸ” æ³¨ç›®é ˜åŸŸ ({left}, {top}) ã‹ã‚‰ ({right}, {bottom}) ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã€æœ€é©ãªãƒ†ã‚­ã‚¹ãƒˆé…ç½®ä½ç½®ã‚’æ¢ã—ã¾ã™...")

        for i in range(left, right):
            for j in range(top, bottom):
                crop_left = max(0, i - text_half_width)
                crop_top = max(0, j - text_half_height)
                crop_right = min(img_width, i + text_half_width)
                crop_bottom = min(img_height, j + text_half_height)
                
                if crop_left < crop_right and crop_top < crop_bottom:
                    patch_pil = generated_pil_image.crop((crop_left, crop_top, crop_right, crop_bottom))
                    patch_gray = patch_pil.convert("L")
                    patch_np = np.array(patch_gray)
                    
                    if patch_np.size == 0:
                        continue

                    intensity_sum = np.sum(patch_np)
                    intensity_std = np.std(patch_np)
                    distance = abs(saliency_center_x - i) + abs(saliency_center_y - j)

                    if intensity_std < 1:
                        continue

                    current_score = intensity_sum + intensity_std + distance * 10
                    
                    if (current_score < flagVar):
                        flagVar = current_score
                        final_x, final_y = i, j

        print(f"âœ… ã‚¹ã‚­ãƒ£ãƒ³å®Œäº†ã€‚")
        print(f"ğŸ’¡ æœ€é©ãªãƒ†ã‚­ã‚¹ãƒˆé…ç½®åº§æ¨™ (x, y): ({final_x}, {final_y})")

        # ----- 3. å…ƒç”»åƒã«è§£ç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’æç”» -----
        
        offset_height = 6
        text_x = final_x - (text_width / 2)
        text_y = final_y - (text_height / 2) + offset_height
        
        # ãƒ†ã‚­ã‚¹ãƒˆãŒç”»åƒã‹ã‚‰ã¯ã¿å‡ºã•ãªã„ã‚ˆã†ã«åº§æ¨™ã‚’æœ€çµ‚èª¿æ•´
        
        # å·¦ç«¯ã®ãƒã‚§ãƒƒã‚¯ã¨èª¿æ•´
        if text_x < 0:
            text_x = 0
        
        # å³ç«¯ã®ãƒã‚§ãƒƒã‚¯ã¨èª¿æ•´
        if text_x + text_width > img_width:
            text_x = img_width - text_width
            
        # ä¸Šç«¯ã®ãƒã‚§ãƒƒã‚¯ã¨èª¿æ•´
        if text_y < 0:
            text_y = 0
            
        # ä¸‹ç«¯ã®ãƒã‚§ãƒƒã‚¯ã¨èª¿æ•´
        if text_y + text_height > img_height:
            text_y = img_height - text_height
            
        # ãƒ†ã‚­ã‚¹ãƒˆæœ¬ä½“ã‚’æç”»
        fill_color = "#00ff00"
        stroke_color = "black"
        stroke_width = 3
        draw.text(
            (text_x, text_y),
            answer_text,
            font=font, 
            fill=fill_color,
            stroke_width=stroke_width,
            stroke_fill=stroke_color
        )

        # ----- 4. æœ€çµ‚ç”»åƒã‚’ä¿å­˜ -----
        final_output_filename = r"example/final_result_with_answer.png"
        generated_pil_image.save(final_output_filename)
        print(f"ğŸ‰ å®Œæˆï¼è§£ç­”ãƒ†ã‚­ã‚¹ãƒˆä»˜ãç”»åƒã‚’ '{final_output_filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")

