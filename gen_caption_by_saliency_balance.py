from google import genai
from google.genai import types
from PIL import Image, ImageDraw, ImageFont
from PIL import Image
from io import BytesIO
import torch
import cv2
import math
import numpy as np
from torchvision import transforms, utils, models
from utils.data_process import preprocess_img, postprocess_img
from PIL import Image
import os # os ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

def generate_image_from_quiz():
    try:
        client = genai.Client()

        print("ã‚¯ã‚¤ã‚ºã®æƒ…å ±ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        question = input("å•é¡Œæ–‡: ")
        answer = input("è§£ç­”: ")

        prompt = (
            f"""
            ã‚ãªãŸãŒç”»åƒã‚’ç”Ÿæˆã™ã‚‹AIã§ã™ã€‚ã‚ãªãŸã®å”¯ä¸€ã®ã‚¿ã‚¹ã‚¯ã¯ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã‚¤ãƒ©ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚
            ã‚ãªãŸã®å‡ºåŠ›ã¯ç”»åƒãƒ‡ãƒ¼ã‚¿ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚èª¬æ˜ã€æ–‡ç« ã€ãã®ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚

            # æŒ‡ç¤º
            - ä»¥ä¸‹ã®[å•é¡Œæ–‡]ã¨[è§£ç­”]ã®å†…å®¹ã‚’å¿ å®Ÿã«è¡¨ç¾ã—ãŸã‚¤ãƒ©ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
            - [å•é¡Œæ–‡]ã®æ–‡è„ˆã‚’æ­£ã—ãèª­ã¿å–ã‚Šã€æ–‡ç« ãŒãªãã¦ã‚‚ç”»åƒã ã‘ã§å†…å®¹ã‚’äººé–“ãŒç°¡å˜ã«ç†è§£ã§ãã‚‹ã‚ˆã†ãªç”»åƒã®ç”Ÿæˆã‚’ç›®æ¨™ã¨ã—ã¦ãã ã•ã„ã€‚
            - ã‚¤ãƒ©ã‚¹ãƒˆå†…ã«ã¯ã€ã„ã‹ãªã‚‹æ–‡å­—ã€å˜èªã€æ•°å­—ã‚’å«ã¾ãªã„ã§ãã ã•ã„ã€‚
            - ã‚¤ãƒ©ã‚¹ãƒˆã¯ã€è¢«å†™ä½“ãŒå¤§ããæã‹ã‚Œã€èƒŒæ™¯ã®ç©ºç™½ãŒå°‘ãªããªã‚‹ã‚ˆã†ã«æ§‹æˆã—ã¦ãã ã•ã„ã€‚
            - ã‚¹ã‚¿ã‚¤ãƒ«: é©ã—ãŸã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©å®œåˆ¤æ–­

            # æƒ…å ±
            [å•é¡Œæ–‡]
            {question}

            [è§£ç­”]
            {answer}
            """
        )
        print("\nç”»åƒã‚’ç”Ÿæˆä¸­ã§ã™...ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚")

        response = client.models.generate_content(
            model="gemini-2.5-flash-image",
            contents=[prompt],
        )

        # ä¿®æ­£: response ã¨ response.candidates ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹
        if response and response.candidates:
            image_found = False
            for part in response.candidates[0].content.parts:
                if part.inline_data is not None:
                    # ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®å‡¦ç†
                    image = Image.open(BytesIO(part.inline_data.data)).convert("RGB")
                    output_filename = r"example/generated_image.png"
                    image.save(output_filename)
                    print(f"GeminiãŒç”Ÿæˆã—ãŸç”»åƒã‚’'{output_filename}'ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ")
                    image_found = True
                    return image, answer

            # ãƒ«ãƒ¼ãƒ—ãŒçµ‚äº†ã—ã¦ã‚‚ç”»åƒãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆ
            if not image_found:
                print("âŒ å¿œç­”ã«ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚")
                # ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ãŒã‚ã‚Œã°è¡¨ç¤ºã™ã‚‹ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                if response.candidates[0].content.parts and response.candidates[0].content.parts[0].text:
                    print(f"ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”: {response.candidates[0].content.parts[0].text}")
                return None, None
        else:
            # responseè‡ªä½“ãŒç„¡åŠ¹ã ã£ãŸå ´åˆ
            print("âŒ ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªå¿œç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None, None

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None, None

def distance(x, y, i, j):
    return int(math.sqrt((x - i)**2 + (y - j)**2)) # è·é›¢è¨ˆç®—é–¢æ•°ã‚’ä¿®æ­£

if __name__ == "__main__":

    generated_pil_image, answer_text = generate_image_from_quiz()

    if generated_pil_image is None:
        print("ç”»åƒã®ç”Ÿæˆã«å¤±æ•—ã—ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
    else:
        generated_image_path = "example/generated_image.png" # generate_image_from_quizã®å‡ºåŠ›ãƒ‘ã‚¹ã¨ä¸€è‡´ã•ã›ã‚‹
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        print(f"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")

        flag = 1 # 0 for TranSalNet_Dense, 1 for TranSalNet_Res

        if flag:
            from TranSalNet_Res import TranSalNet
            model = TranSalNet()
            model.load_state_dict(torch.load(r'pretrained_models\TranSalNet_Res.pth'))
        else:
            from TranSalNet_Dense import TranSalNet
            model = TranSalNet()
            model.load_state_dict(torch.load(r'pretrained_models\TranSalNet_Dense.pth'))

        # --- 2-1. Saliency Mapè¨ˆç®—ã®æº–å‚™ ---
        model = model.to(device) 
        model.eval()

        # --- 2-2. å…¥åŠ›ç”»åƒã®å‰å‡¦ç† ---
        img = preprocess_img(generated_image_path)
        img = np.array(img)/255.
        img = np.expand_dims(np.transpose(img,(2,0,1)),axis=0)

        # --- 2-3. PyTorchãƒ†ãƒ³ã‚½ãƒ«ã¸ã®å¤‰æ›ã¨æ¨è«– ---
        img = torch.from_numpy(img).type(torch.cuda.FloatTensor).to(device)
        pred_saliency_tensor = model(img)

        # --- 2-4. å‡ºåŠ›ã®å¾Œå‡¦ç† ---
        toPIL = transforms.ToPILImage()
        pic = toPIL(pred_saliency_tensor.squeeze())
        saliency_map_np = postprocess_img(pic, generated_image_path)
        
        # --- 2-5. é¡•è‘—æ€§ãƒãƒƒãƒ—ã®ä¿å­˜ ---
        saliency_output_filename = r'example/result_saliency.png'
        cv2.imwrite(saliency_output_filename, saliency_map_np, [int(cv2.IMWRITE_JPEG_QUALITY), 100])
        print(f"Saliency Mapã‚’ '{saliency_output_filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")

        # --- 2-6. Saliency Mapã®æœ€å¤§å€¤ï¼ˆæœ€ã‚‚ç›®ç«‹ã¤ç‚¹ï¼‰ã‚’å–å¾— ---
        max_loc_flat = np.argmax(saliency_map_np)
        max_y, max_x = np.unravel_index(max_loc_flat, saliency_map_np.shape)
        print(f"ğŸ’¡ Saliency Mapã§æœ€ã‚‚æ³¨è¦–ã•ã‚Œã‚‹åº§æ¨™ (x, y): ({max_x}, {max_y})")

        saliency_center_x = int(max_x)
        saliency_center_y = int(max_y)

        # --- ãƒ†ã‚­ã‚¹ãƒˆæç”»ã®æº–å‚™ ---
        square_size = 100
        half_size = square_size // 2

        # Saliencyä¸­å¿ƒã‹ã‚‰æ¢ç´¢ã™ã‚‹é ˜åŸŸã‚’å®šç¾©
        left = saliency_center_x - half_size
        top = saliency_center_y - half_size
        right = saliency_center_x + half_size
        bottom = saliency_center_y + half_size

        draw = ImageDraw.Draw(generated_pil_image)
        
        # ãƒ•ã‚©ãƒ³ãƒˆã¨ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã®æº–å‚™
        font_path = "C:/Windows/Fonts/meiryob.ttc"
        font_size = 36
        try:
            font = ImageFont.truetype(font_path, font_size)
        except IOError:
            print(f"è­¦å‘Š: æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ³ãƒˆ '{font_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            font = ImageFont.load_default()
        
        text_bbox = draw.textbbox((0, 0), answer_text, font=font)
        text_width = text_bbox[2] - text_bbox[0]
        text_height = text_bbox[3] - text_bbox[1]
        
        img_width, img_height = generated_pil_image.size
        
        # --- 2-7. Saliency Mapã®å‹¾é…ãŒæœ€å¤§ã¨ãªã‚‹ä½ç½®ã‚’æ¢ç´¢ ---
        print(f"\nğŸ” Saliency Mapã®æ³¨ç›®é ˜åŸŸ ({left}, {top}) ã‹ã‚‰ ({right}, {bottom}) ã®å‹¾é…ã‚’è¨ˆç®—ã—ã¾ã™...")

        # Saliency Mapã®ã‚µã‚¤ã‚ºã‚’å–å¾— (NumPyã¯ H, W ã®é †)
        saliency_height, saliency_width = saliency_map_np.shape

        # åº§æ¨™ãŒç”»åƒå¢ƒç•Œã‚’ã¯ã¿å‡ºã•ãªã„ã‚ˆã†ã«èª¿æ•´ (ã‚¯ãƒ­ãƒƒãƒ—åº§æ¨™ã‚’æ±ºå®š)
        crop_left = max(0, left)
        crop_top = max(0, top)
        crop_right = min(saliency_width, right)
        crop_bottom = min(saliency_height, bottom)

        # æ³¨ç›®é ˜åŸŸï¼ˆSaliencyã®ä¸­å¿ƒå‘¨è¾ºï¼‰ã‚’åˆ‡ã‚Šå‡ºã™
        # NumPyã®ã‚¹ãƒ©ã‚¤ã‚¹ [y1:y2, x1:x2]
        saliency_crop = saliency_map_np[crop_top:crop_bottom, crop_left:crop_right]

        if saliency_crop.size == 0:
            print("è­¦å‘Š: Saliency Mapã®ã‚¯ãƒ­ãƒƒãƒ—é ˜åŸŸãŒç©ºã§ã™ã€‚ä¸­å¿ƒåº§æ¨™ã‚’ãã®ã¾ã¾ä½¿ã„ã¾ã™ã€‚")
            final_x, final_y = saliency_center_x, saliency_center_y
        else:
            # Saliency Mapã®Xæ–¹å‘ã¨Yæ–¹å‘ã®å‹¾é…ã‚’è¨ˆç®— (cv2.CV_64Fã§è² ã®å€¤ã‚‚è€ƒæ…®)
            grad_x_full = cv2.Sobel(saliency_map_np, cv2.CV_64F, 1, 0, ksize=5) # å…¨ä½“ã§è¨ˆç®—
            grad_y_full = cv2.Sobel(saliency_map_np, cv2.CV_64F, 0, 1, ksize=5) # ksizeã‚’5ã«èª¿æ•´ã—ã¦å¹³æ»‘åŒ–

            # SaliencyãŒæœ€ã‚‚å¤§ãã„ç‚¹ã§ã®å‹¾é…ãƒ™ã‚¯ãƒˆãƒ«
            main_grad_x = grad_x_full[saliency_center_y, saliency_center_x]
            main_grad_y = grad_y_full[saliency_center_y, saliency_center_x]
            
            # å‹¾é…ã®å¤§ãã•ãŒ0ã«è¿‘ã„å ´åˆã¯å›é¿ (ä¾‹: å®Œå…¨ã«å¹³å¦ãªSaliencyã®å ´åˆ)
            grad_magnitude_at_center = math.sqrt(main_grad_x**2 + main_grad_y**2)

            # ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã®å¤§ãã•ã®ç›®å®‰
            text_box_diag_length = math.sqrt(text_width**2 + text_height**2)
            # é…ç½®ã‚ªãƒ•ã‚»ãƒƒãƒˆé‡ã‚’èª¿æ•´ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã®å¯¾è§’ç·šã®åŠåˆ†ç¨‹åº¦ã‚’åŸºæº–ã«ï¼‰
            # SaliencyãŒæ¸›ã‚‹æ–¹å‘ã«ç§»å‹•ã•ã›ãŸã„ã®ã§ã€å‹¾é…ã®åå¯¾æ–¹å‘ã¸
            offset_factor = text_box_diag_length * 0.75 # 0.75ã¯èª¿æ•´å¯èƒ½

            if grad_magnitude_at_center > 0.1: # å‹¾é…ãŒååˆ†ã«å¤§ãã„å ´åˆ
                # å‹¾é…ã®æ­£è¦åŒ–ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆSaliencyãŒå¢—åŠ ã™ã‚‹æ–¹å‘ï¼‰
                normalized_grad_x = main_grad_x / grad_magnitude_at_center
                normalized_grad_y = main_grad_y / grad_magnitude_at_center

                # SaliencyãŒæ¸›å°‘ã™ã‚‹æ–¹å‘ã«ã‚ªãƒ•ã‚»ãƒƒãƒˆ
                offset_x = -normalized_grad_x * offset_factor
                offset_y = -normalized_grad_y * offset_factor

                # åˆæœŸé…ç½®å€™è£œç‚¹ (saliency_center_x, saliency_center_y) ã«ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’åŠ ç®—
                final_x = int(saliency_center_x + offset_x)
                final_y = int(saliency_center_y + offset_y)
            else:
                # å‹¾é…ãŒå°ã•ã„å ´åˆã¯Saliencyä¸­å¿ƒã‹ã‚‰å°‘ã—ãšã‚‰ã™ãªã©ã€åˆ¥ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ¤œè¨
                # ä»Šå›ã¯Saliencyä¸­å¿ƒã‹ã‚‰å°‘ã—å³ä¸‹ã«ã‚ªãƒ•ã‚»ãƒƒãƒˆã™ã‚‹ï¼ˆä¾‹ï¼‰
                print("è­¦å‘Š: Saliencyä¸­å¿ƒã§ã®å‹¾é…ãŒå°ã•ã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                final_x = saliency_center_x + half_size // 2
                final_y = saliency_center_y + half_size // 2

            print(f"âœ… å‹¾é…è¨ˆç®—ã¨ã‚ªãƒ•ã‚»ãƒƒãƒˆå®Œäº†ã€‚")
            print(f"ğŸ’¡ æœ€é©ãªãƒ†ã‚­ã‚¹ãƒˆé…ç½®åº§æ¨™ (x, y): ({final_x}, {final_y})")

        # ----- 3. å…ƒç”»åƒã«è§£ç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’æç”» -----
        
        offset_height = 6 # ãƒ•ã‚©ãƒ³ãƒˆã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å¾®èª¿æ•´ç”¨
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ final_x, final_y ã®ä¸­å¿ƒã«é…ç½®ã™ã‚‹ãŸã‚ã®å·¦ä¸Šåº§æ¨™ã‚’è¨ˆç®—
        text_x = final_x - (text_width / 2)
        text_y = final_y - (text_height / 2) + offset_height
        
        # ãƒ†ã‚­ã‚¹ãƒˆãŒç”»åƒã‹ã‚‰ã¯ã¿å‡ºã•ãªã„ã‚ˆã†ã«åº§æ¨™ã‚’æœ€çµ‚èª¿æ•´
        
        # å·¦ç«¯ã®ãƒã‚§ãƒƒã‚¯ã¨èª¿æ•´
        if text_x < 0:
            text_x = 0
        
        # å³ç«¯ã®ãƒã‚§ãƒƒã‚¯ã¨èª¿æ•´
        if text_x + text_width > img_width:
            text_x = img_width - text_width
            
        # ä¸Šç«¯ã®ãƒã‚§ãƒƒã‚¯ã¨èª¿æ•´
        if text_y < 0:
            text_y = 0
            
        # ä¸‹ç«¯ã®ãƒã‚§ãƒƒã‚¯ã¨èª¿æ•´
        if text_y + text_height > img_height:
            text_y = img_height - text_height
            
        # ãƒ†ã‚­ã‚¹ãƒˆæœ¬ä½“ã‚’æç”»
        fill_color = "#00ff00"
        stroke_color = "black"
        stroke_width = 3
        draw.text(
            (text_x, text_y),
            answer_text,
            font=font, 
            fill=fill_color,
            stroke_width=stroke_width,
            stroke_fill=stroke_color
        )

        # ----- 4. æœ€çµ‚ç”»åƒã‚’ä¿å­˜ -----
        final_output_filename = r"example/final_result_with_answer.png"
        generated_pil_image.save(final_output_filename)
        print(f"ğŸ‰ å®Œæˆï¼è§£ç­”ãƒ†ã‚­ã‚¹ãƒˆä»˜ãç”»åƒã‚’ '{final_output_filename}' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚")